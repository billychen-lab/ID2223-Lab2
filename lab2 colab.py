# -*- coding: utf-8 -*-
"""t0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1C0om6V_6wSmvsP2qXQN8czzbyRq0t2Zf
"""

!pip install -U "transformers[torch]" datasets accelerate







from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset

model_name = "sshleifer/tiny-gpt2"   # 玩具模型，方便测试
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 随便加载一个小数据集
dataset = load_dataset("stas/openwebtext-10k", split="train[:100]")

print(dataset[0])

from datasets import load_dataset

dataset = load_dataset("NeelNanda/pile-10k", split="train[:100]")
print(dataset[0])

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -q "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install -q --no-deps trl transformers accelerate datasets bitsandbytes peft
#

from unsloth import FastLanguageModel, is_bfloat16_supported
import torch

# 1) 一些配置
max_seq_length = 2048        # 上下文长度
dtype = None                 # 让 unsloth 自动选 fp16 / bf16
load_in_4bit = True          # 4bit 量化，省显存

# 2) 选模型：1B 或 3B
model_name = "unsloth/Llama-3.2-3B-Instruct"   # 想换 3B 就改成 3B 的名字

# 3) 从 Hugging Face 加载模型 + tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name      = model_name,
    max_seq_length  = max_seq_length,
    dtype           = dtype,
    load_in_4bit    = load_in_4bit,
    # 如果模型是 gated，可能需要：token="你的HF_TOKEN"
)

# 4) 给模型加 LoRA 适配层（PEFT）
model = FastLanguageModel.get_peft_model(
    model,
    r                         = 16,   # LoRA rank，越大越耗显存
    lora_alpha                = 16,
    lora_dropout              = 0,
    target_modules            = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "up_proj", "down_proj",
    ],
    use_gradient_checkpointing = "unsloth",
    random_state              = 3407,
)

from datasets import load_dataset
from unsloth.chat_templates import get_chat_template, standardize_sharegpt

# 1) 加载 FineTome-100k
dataset = load_dataset("mlabonne/FineTome-100k", split="train")

# 2) 标准化成 role/content 格式
dataset = standardize_sharegpt(dataset)

# 3) 给 tokenizer 加上 Llama 3.1/3.2 的聊天模板
tokenizer = get_chat_template(tokenizer, chat_template="llama-3.1")

# 4) 把 conversations 转成一整段 text
def format_prompts(examples):
    texts = [
        tokenizer.apply_chat_template(
            convo,
            tokenize=False,
            add_generation_prompt=False,
        )
        for convo in examples["conversations"]
    ]
    return {"text": texts}

dataset = dataset.map(format_prompts, batched=True)
print(dataset[0]["text"][:500])

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported
from unsloth.chat_templates import train_on_responses_only

max_seq_length = 2048

# 为了先把流程跑通，只用前 2000 条样本
train_dataset = dataset.select(range(2000))

trainer = SFTTrainer(
    model              = model,
    tokenizer          = tokenizer,
    train_dataset      = train_dataset,
    dataset_text_field = "text",
    max_seq_length     = max_seq_length,
    data_collator      = DataCollatorForSeq2Seq(tokenizer=tokenizer),
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps                = 5,
        max_steps                   = 50,       # 先训练 50 步试流程
        learning_rate               = 2e-4,
        fp16                        = not is_bfloat16_supported(),
        bf16                        = is_bfloat16_supported(),
        logging_steps               = 1,
        output_dir                  = "outputs",
        report_to                   = "none",
        optim                       = "adamw_8bit",
        weight_decay                = 0.01,
        save_steps                  = 10,       # 每 10 步存一个 checkpoint
    ),
)

# 只在 assistant 回复部分算 loss（可选，但推荐）
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part    = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)

trainer.train()

# 训练完保存 LoRA 权重和 tokenizer
trainer.save_model("llama32-3b-finetome-lora")
tokenizer.save_pretrained("llama32-3b-finetome-lora")

from unsloth import FastLanguageModel
import torch

# 让模型进入推理模式（关闭 dropout 等，并用 Unsloth 的 2x 加速）
FastLanguageModel.for_inference(model)

def chat_once(user_prompt, system_prompt="You are a helpful assistant."):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": user_prompt},
    ]

    # 把对话转换成 Llama chat 模板的纯文本
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,  # 让模型知道该轮到 assistant 说话了
    )

    inputs = tokenizer(
        text,
        return_tensors="pt",
    ).to("cuda")   # 在 Colab GPU 上推理

    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )

    # 只取新生成的部分
    generated_ids = outputs[0, inputs["input_ids"].shape[-1]:]
    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
    print(answer)

# 试一条和训练集风格类似的问题
chat_once("用小学生能懂的方式解释一下布尔运算符是什么？")

!ls

from google.colab import drive
drive.mount('/content/drive')

# 在云盘里给这个 lab 建一个文件夹
#!mkdir -p "/content/drive/MyDrive/id2223_lab2"

# 把当前训练出来的 checkpoint 和 LoRA 目录都拷过去
!cp -r outputs "/content/drive/MyDrive/id2223_lab2/outputs_step50_3B"
!cp -r llama32-3b-finetome-lora "/content/drive/MyDrive/id2223_lab2/llama32-3b-finetome-lora"

# 看看里面有什么
!ls "/content/drive/MyDrive/id2223_lab2"

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -q "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install -q --no-deps trl transformers accelerate datasets bitsandbytes peft
#

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/MyDrive/id2223_lab2"

from unsloth import FastLanguageModel, is_bfloat16_supported
from datasets import load_dataset
from unsloth.chat_templates import get_chat_template, standardize_sharegpt
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from trl import SFTTrainer

max_seq_length = 2048
dtype = None
load_in_4bit = True

adapter_dir = "/content/drive/MyDrive/id2223_lab2/llama32-3b-finetome-lora"
ckpt_dir    = "/content/drive/MyDrive/id2223_lab2/outputs_step50_3B"

# 1) 从云盘里的 LoRA 目录加载模型
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = adapter_dir,
    max_seq_length = max_seq_length,
    dtype          = dtype,
    load_in_4bit   = load_in_4bit,
)

# 2) 重新构建 FineTome 数据集（跟第一次训练时一样）
dataset = load_dataset("mlabonne/FineTome-100k", split="train")
dataset = standardize_sharegpt(dataset)
tokenizer = get_chat_template(tokenizer, chat_template="llama-3.1")

def format_prompts(examples):
    texts = [
        tokenizer.apply_chat_template(
            convo,
            tokenize=False,
            add_generation_prompt=False,
        )
        for convo in examples["conversations"]
    ]
    return {"text": texts}

dataset = dataset.map(format_prompts, batched=True)
train_dataset = dataset.select(range(2000))   # 先继续用同样的 2000 条

# 3) 定义 Trainer，这次想把训练步数拉到 100 步
trainer = SFTTrainer(
    model              = model,
    tokenizer          = tokenizer,
    train_dataset      = train_dataset,
    dataset_text_field = "text",
    max_seq_length     = max_seq_length,
    data_collator      = DataCollatorForSeq2Seq(tokenizer=tokenizer),
    args = TrainingArguments(
        per_device_train_batch_size = 2,
        gradient_accumulation_steps = 4,
        warmup_steps                = 5,
        max_steps                   = 100,      # <== 之前是 50，这里改成 100
        learning_rate               = 2e-4,
        fp16                        = not is_bfloat16_supported(),
        bf16                        = is_bfloat16_supported(),
        logging_steps               = 5,
        output_dir                  = ckpt_dir, # <== 直接写到云盘里的 outputs_step50
        report_to                   = "none",
        optim                       = "adamw_8bit",
        weight_decay                = 0.01,
        save_steps                  = 10,
    ),
)

# 4) 关键：从 step 50 的 checkpoint 继续训练
trainer.train(resume_from_checkpoint=f"{ckpt_dir}/checkpoint-50")

# 5) 训练完再把 LoRA+tokenizer 保存回同一个目录（覆盖原来的）
trainer.save_model(adapter_dir)
tokenizer.save_pretrained(adapter_dir)

!ls "/content/drive/MyDrive/id2223_lab2/outputs_step50_3B"

from unsloth import FastLanguageModel

FastLanguageModel.for_inference(model)

def chat_once(user_prompt, system_prompt="You are a helpful assistant."):
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user",   "content": user_prompt},
    ]

    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )

    inputs = tokenizer(text, return_tensors="pt").to("cuda")
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
    )

    generated_ids = outputs[0, inputs["input_ids"].shape[-1]:]
    answer = tokenizer.decode(generated_ids, skip_special_tokens=True)
    print(answer)

chat_once("再用小学生能懂的方式解释一下布尔运算符是什么？")

# 把 LoRA 合并成一个完整模型，存在本地
merged_dir = "/content/llama32-3b-finetome-merged16"

model.save_pretrained_merged(
    merged_dir,
    tokenizer,
    save_method = "merged_16bit",   # 得到一个 FP16 的完整模型
)

!ls "$merged_dir"

!cp -r "/content/llama32-3b-finetome-merged16" "/content/drive/MyDrive/id2223_lab2/"

from huggingface_hub import login
login()   # 粘贴 token 回车

HF_USERNAME = "yunquan01"   # 比如 'yunquan'，在 HF 右上角头像旁边那个名字

LORA_REPO   = f"{HF_USERNAME}/llama32-3b-finetome-lora"
MERGED_REPO = f"{HF_USERNAME}/llama32-3b-finetome-merged16"
print(LORA_REPO, MERGED_REPO)

from unsloth import FastLanguageModel

# （1）先上传 LoRA 版本（体积小）
model.push_to_hub(
    LORA_REPO,
    tokenizer,
)

# （2）再上传合并后的 16bit 模型（如果你之前已经 save_pretrained_merged）
model.push_to_hub_merged(
    MERGED_REPO,
    tokenizer,
    save_method = "merged_16bit",
)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -q "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install -q --no-deps trl transformers accelerate datasets bitsandbytes peft
# 
# from huggingface_hub import login
# login()   # 粘贴你之前的 hf_token
#

from unsloth import FastLanguageModel

max_seq_length = 2048
dtype = None
load_in_4bit = True

adapter_repo = "yunquan01/llama32-3b-finetome-lora"  # 你 LoRA 模型的 HF 仓库
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = adapter_repo,
    max_seq_length = max_seq_length,
    dtype          = dtype,
    load_in_4bit   = load_in_4bit,
)

GGUF_REPO = "yunquan01/llama32-1b-finetome-gguf"   # 你想要的新仓库名

model.push_to_hub_gguf(
    GGUF_REPO,
    tokenizer,
    quantization_method = ["q4_k_m"],  # 经典 4-bit 量化，CPU 友好
)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install -q "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install -q --no-deps trl transformers accelerate datasets bitsandbytes peft
# 
# from huggingface_hub import login
# login()   # 粘贴你的 HF token
#

from unsloth import FastLanguageModel

max_seq_length = 1024      # 比之前 2048 小一点，省内存
dtype = None
load_in_4bit = True

adapter_repo = "yunquan01/llama32-1b-finetome-lora"

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = adapter_repo,
    max_seq_length = max_seq_length,
    dtype          = dtype,
    load_in_4bit   = load_in_4bit,
)

GGUF_REPO = "yunquan01/llama32-1b-finetome-gguf"

model.push_to_hub_gguf(
    GGUF_REPO,
    tokenizer,
    quantization_method = "q4_k_m",   # 或者换 "q4_0" 更轻一点
)

!pip install -q transformers accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
device

MODEL_SMALL = "yunquan01/llama32-1b-finetome-merged16"
MODEL_BIG   = "yunquan01/llama32-3b-finetome-merged16"

def load_model(repo_id):
    tok = AutoTokenizer.from_pretrained(repo_id)
    mod = AutoModelForCausalLM.from_pretrained(
        repo_id,
        torch_dtype = torch.float16 if device == "cuda" else torch.float32,
    )
    mod.to(device)
    mod.eval()
    return tok, mod

small_tok, small_model = load_model(MODEL_SMALL)
big_tok,   big_model   = load_model(MODEL_BIG)

import time

SYSTEM_PROMPT = (
    "You are a helpful assistant fine-tuned on the FineTome instruction dataset. "
    "Answer clearly and concisely."
)

def chat_once(tokenizer, model, user_message, max_new_tokens=128):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user_message},
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = tokenizer(text, return_tensors="pt").to(device)

    with torch.no_grad():
        t0 = time.time()
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
        dt = time.time() - t0

    gen_ids = outputs[0, inputs["input_ids"].shape[-1]:]
    answer = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return answer, dt

questions = {
    "Q1": "用小学生能懂的方式解释一下布尔运算符是什么？",
    "Q2": "“与（AND）” 和 “或（OR）” 在布尔运算里有什么区别？请用生活中的例子说明。",
    "Q3": "什么是“非（NOT）”运算？举两个例子。",
    "Q4": "在编程里，if (x > 3 && x < 10) 这句条件是什么意思？",
    "Q5": "在编程里，if (age < 18 || isStudent) 这句条件是什么意思？",
    # ... 可继续加到 Q10、Q15
}
results = {}

for qid, qtext in questions.items():
    print("="*80)
    print(qid, qtext)

    small_ans, small_t = chat_once(small_tok, small_model, qtext)
    big_ans,   big_t   = chat_once(big_tok,   big_model,   qtext)

    results[qid] = {
        "question": qtext,
        "small_answer": small_ans,
        "small_time": small_t,
        "big_answer": big_ans,
        "big_time": big_t,
    }

    print("\n[1B model answer] (time: %.2fs)\n" % small_t, small_ans)
    print("\n[3B model answer] (time: %.2fs)\n" % big_t, big_ans)

!pip install -q transformers accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

device = "cuda" if torch.cuda.is_available() else "cpu"
device

MODEL_SMALL = "yunquan01/llama32-1b-finetome-merged16"
MODEL_BIG   = "yunquan01/llama32-3b-finetome-merged16"

def load_model(repo_id):
    tok = AutoTokenizer.from_pretrained(repo_id)
    mod = AutoModelForCausalLM.from_pretrained(
        repo_id,
        torch_dtype=torch.float16 if device=="cuda" else torch.float32,
    )
    mod.to(device)
    mod.eval()
    return tok, mod

small_tok, small_model = load_model(MODEL_SMALL)
big_tok,   big_model   = load_model(MODEL_BIG)

import time

SYSTEM_PROMPT = (
    "You are a helpful assistant fine-tuned on the FineTome instruction dataset. "
    "Answer clearly and concisely."
)

def chat_once(tokenizer, model, user_message, max_new_tokens=128):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user_message},
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    inputs = tokenizer(text, return_tensors="pt").to(device)

    with torch.no_grad():
        t0 = time.time()
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
        )
        dt = time.time() - t0

    gen_ids = outputs[0, inputs["input_ids"].shape[-1]:]
    answer = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return answer, dt

questions = {
    "Q1": "用小学生能懂的方式解释一下布尔运算符是什么？",
    "Q2": "“与（AND）” 和 “或（OR）” 在布尔运算里有什么区别？请用生活中的例子说明。",
    "Q3": "什么是“非（NOT）”运算？举两个例子。",
    "Q4": "在编程里，if (x > 3 && x < 10) 这句条件是什么意思？",
    "Q5": "在编程里，if (age < 18 || isStudent) 这句条件是什么意思？",
    # ... 继续加 Q3 ~ Q10
}

results = {}

for qid, qtext in questions.items():
    print("="*80)
    print(qid, qtext)

    small_ans, small_t = chat_once(small_tok, small_model, qtext)
    big_ans,   big_t   = chat_once(big_tok,   big_model,   qtext)

    results[qid] = {
        "question": qtext,
        "small_answer": small_ans,
        "small_time": small_t,
        "big_answer": big_ans,
        "big_time": big_t,
    }

    print("\n[1B model answer] (time: %.2fs)\n" % small_t, small_ans)
    print("\n[3B model answer] (time: %.2fs)\n" % big_t, big_ans)

# 把所有问题和回答保存成一个 txt 文件
with open("boolean_eval_results.txt", "w", encoding="utf-8") as f:
    for qid, r in results.items():
        f.write(f"{qid} {r['question']}\n")
        f.write(f"[1B model answer] (time: {r['small_time']:.2f}s)\n")
        f.write(r["small_answer"] + "\n\n")
        f.write(f"[3B model answer] (time: {r['big_time']:.2f}s)\n")
        f.write(r["big_answer"] + "\n")
        f.write("="*80 + "\n\n")

from google.colab import files
files.download("boolean_eval_results.txt")

import time
SYSTEM_PROMPT=("You are a good model"
"Answer clearly and concisely"
)
model = "yunquan01/llama32-1b-finetome-merged16"


questions = {
    "Q1": "用小学生能懂的方式解释一下布尔运算符是什么？",
    "Q2": "“与（AND）” 和 “或（OR）” 在布尔运算里有什么区别？请用生活中的例子说明。",
    "Q3": "什么是“非（NOT）”运算？举两个例子。",
    "Q4": "在编程里，if (x > 3 && x < 10) 这句条件是什么意思？",
    "Q5": "在编程里，if (age < 18 || isStudent) 这句条件是什么意思？",
    # ... 继续加 Q3 ~ Q10
}
def load_model(repo_id):
    tok = AutoTokenizer.from_pretrained(repo_id)
    mod = AutoModelForCausalLM.from_pretrained(
        repo_id,
        torch_dtype=torch.float16 if device=="cuda" else torch.float32,
    )
    mod.to(device)
    mod.eval()
    return tok, mod

tok, model = load_model(model)
def chat_once_with_params(tokenizer, model, user_message,
                          temperature, top_p, max_new_tokens=128):
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user_message},
    ]
    text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True,
    )
    inputs = tokenizer(text, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=top_p,
            pad_token_id=tokenizer.eos_token_id,
        )

    gen_ids = outputs[0, inputs["input_ids"].shape[-1]:]
    answer = tokenizer.decode(gen_ids, skip_special_tokens=True)
    return answer
qs = [questions["Q1"], questions["Q2"], questions["Q3"]]  # 你写成自己的问题字符串


for q in qs:
    ans_A = chat_once_with_params(tok, model, q, temperature=0.2, top_p=0.7)
    ans_B = chat_once_with_params(tok, model, q, temperature=0.7, top_p=0.9)

    print("Question:", q)
    print("[Setting A] temp=0.2, top_p=0.7\n", ans_A, "\n")
    print("[Setting B] temp=0.7, top_p=0.9\n", ans_B)
    print("="*80)

MODEL_SMALL = "yunquan01/llama32-1b-finetome-merged16"
MODEL_BIG   = "yunquan01/llama32-3b-finetome-merged16"

def load_model(repo_id):
    tok = AutoTokenizer.from_pretrained(repo_id)
    mod = AutoModelForCausalLM.from_pretrained(
        repo_id,
        torch_dtype=torch.float16 if device=="cuda" else torch.float32,
    )
    mod.to(device)
    mod.eval()
    return tok, mod

small_tok, small_model = load_model(MODEL_SMALL)
big_tok,   big_model   = load_model(MODEL_BIG)